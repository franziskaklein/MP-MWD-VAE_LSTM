{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import math \n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "from sklearn.model_selection import train_test_split # for splitting the data\n",
    "from sklearn.metrics import mean_squared_error # for calculating the cost function\n",
    "from sklearn.ensemble import RandomForestRegressor # for building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjust display of dataframe within jupyter notebook\n",
    "pd.set_option('display.max_rows', 32)\n",
    "pd.set_option('display.max_columns', 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(a, f):\n",
    "    return 1/len(a) * np.sum(2 * np.abs(f-a) / (np.abs(a) + np.abs(f))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.read_csv('raw_mwd_explosives_Franziska.csv')\n",
    "data_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define which hole types to use\n",
    "count = 0\n",
    "for group_name, group_df in data_raw.groupby('holeID'):\n",
    "    if 0.0 not in group_df['Hole type'].values:\n",
    "        count +=1\n",
    "print(f\"In {count} groups there is no type 0.\")\n",
    "\n",
    "count = 0\n",
    "for group_name, group_df in data_raw.groupby('holeID'):\n",
    "    if 2.0 not in group_df['Hole type'].values:\n",
    "        count +=1\n",
    "print(f\"In {count} groups there is no type 2.\")\n",
    "\n",
    "count = 0\n",
    "for group_name, group_df in data_raw.groupby('holeID'):\n",
    "    if 3.0 not in group_df['Hole type'].values:\n",
    "        count +=1\n",
    "print(f\"In {count} groups there is no type 3.\")\n",
    "\n",
    "count = 0\n",
    "for group_name, group_df in data_raw.groupby('holeID'):\n",
    "    if 4.0 not in group_df['Hole type'].values:\n",
    "        count +=1\n",
    "print(f\"In {count} groups there is no type 4.\")\n",
    "\n",
    "count = 0\n",
    "for group_name, group_df in data_raw.groupby('holeID'):\n",
    "    if 5.0 not in group_df['Hole type'].values:\n",
    "        count +=1\n",
    "print(f\"In {count} groups there is no type 5.\")\n",
    "\n",
    "count = 0\n",
    "for group_name, group_df in data_raw.groupby('holeID'):\n",
    "    if 6.0 not in group_df['Hole type'].values:\n",
    "        count +=1\n",
    "print(f\"In {count} groups there is no type 6.\")\n",
    "\n",
    "count = 0\n",
    "for group_name, group_df in data_raw.groupby('holeID'):\n",
    "    if 7.0 not in group_df['Hole type'].values:\n",
    "        count +=1\n",
    "print(f\"In {count} groups there is no type 7.\")\n",
    "\n",
    "count = 0\n",
    "for group_name, group_df in data_raw.groupby('holeID'):\n",
    "    if 8.0 not in group_df['Hole type'].values:\n",
    "        count +=1\n",
    "print(f\"In {count} groups there is no type 8.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use just hole type 0\n",
    "data_sorted = data_raw.loc[data_raw['Hole type'] == 0.0].copy()\n",
    "\n",
    "#sort according to location of the hole\n",
    "\n",
    "count = 0\n",
    "for group_name, group_df in data_sorted.groupby('holeID'):\n",
    "    if \"upper\" not in group_df['location'].values:\n",
    "        count +=1\n",
    "print(f\"In {count} groups there is no borehole located in upper.\")\n",
    "\n",
    "count = 0\n",
    "for group_name, group_df in data_sorted.groupby('holeID'):\n",
    "    if \"buttom\" not in group_df['location'].values:\n",
    "        count +=1\n",
    "print(f\"In {count} groups there is no borehole located in buttom.\")\n",
    "\n",
    "count = 0\n",
    "for group_name, group_df in data_sorted.groupby('holeID'):\n",
    "    if \"upper & bottom\" not in group_df['location'].values:\n",
    "        count +=1\n",
    "print(f\"In {count} groups there is no borehole located in upper & bottom.\")\n",
    "\n",
    "#as in only 1700 groups there is no borehole located in upper & bottom, this is chosen\n",
    "data_sorted = data_sorted.loc[data_raw['location'] == \"upper & bottom\"].copy()\n",
    "data_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split: five for test, ten for train (<= nuique_holeIDs_per_faceN)\n",
    "\n",
    "def faces_holes_df(dataframe):\n",
    "    #get all possible combinations of face and hole\n",
    "    unique_combinations = dataframe[['faceN', 'holeID']].drop_duplicates()\n",
    "    \n",
    "    #create df with just faces and holes\n",
    "    faces_to_holes = pd.DataFrame(columns=['faceN', 'holeID'])\n",
    "    faces_to_holes['faceN'] = unique_combinations['faceN']\n",
    "    faces_to_holes['holeID'] = unique_combinations['holeID']\n",
    "    \n",
    "    return faces_to_holes\n",
    "\n",
    "#get all faces with their holes\n",
    "faces_holes = faces_holes_df(data_sorted)\n",
    "\n",
    "#check minimum number of chosen holes per face\n",
    "unique_holeIDs_per_faceN = faces_holes.groupby('faceN')['holeID'].nunique()\n",
    "\n",
    "print(f'minimal number of holes per face {unique_holeIDs_per_faceN.min()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split boreholes in train and test\n",
    "\n",
    "def train_test_split(dataframe):\n",
    "    train_holes = pd.DataFrame(columns=['faceN', 'holeID'])\n",
    "    test_holes = pd.DataFrame(columns=['faceN', 'holeID'])\n",
    "\n",
    "    # go through different faceN and use first and second value to select holeID for train test split\n",
    "    for faceN, group in dataframe.groupby('faceN'):\n",
    "        train_holes_batch = group['holeID'].iloc[:10]  #first ten holes per face for training\n",
    "        test_holes_batch = group['holeID'].iloc[10:15] #next five holes for test\n",
    "\n",
    "        train_holes_batch_df = pd.DataFrame({'faceN': faceN, 'holeID': train_holes_batch})\n",
    "        test_holes_batch_df = pd.DataFrame({'faceN': faceN, 'holeID': test_holes_batch})\n",
    "        \n",
    "        train_holes = pd.concat([train_holes, train_holes_batch_df], ignore_index=True)\n",
    "        test_holes = pd.concat([test_holes, test_holes_batch_df], ignore_index=True)\n",
    "    \n",
    "    return train_holes, test_holes\n",
    "\n",
    "train_holes = pd.DataFrame(columns=['faceN', 'holeID'])\n",
    "test_holes = pd.DataFrame(columns=['faceN', 'holeID'])\n",
    "\n",
    "train_holes, test_holes = train_test_split(faces_holes)\n",
    "print(\"Trainholes\")\n",
    "print(train_holes)\n",
    "print(\"Testholes\")\n",
    "print(test_holes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data in groups and assign timestamp 0 to first timestep in each group\n",
    "from datetime import datetime\n",
    "data = data_sorted.copy()\n",
    "\n",
    "data['timestamp'] = None #new timestamp for timeseries\n",
    "data['Time'] = pd.to_datetime(data['Time'], format='%H:%M:%S') #to calculate with time\n",
    "\n",
    "def assign_timestamp(group):\n",
    "    first_time = group['Time'].iloc[0]\n",
    "    group['timestamp'] = (group['Time'] - first_time).dt.total_seconds()\n",
    "    return group\n",
    "\n",
    "#add timestamps for all groups\n",
    "data_lstm = data.groupby(\"holeID\", group_keys=False).apply(assign_timestamp).reset_index(drop=True)\n",
    "data_lstm['timestamp'] = data_lstm['timestamp'].astype(int)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interpolate values such that series is equally spaces --> needed for LSTM\n",
    "\n",
    "def interpolate_rows(group):\n",
    "    \n",
    "    # number of timestamps per group\n",
    "    timestamps =  group['timestamp'].iloc[-1] + 1 \n",
    "\n",
    "    missing_rows = []\n",
    "    existing_timestamps = group['timestamp']\n",
    "    existing_timestamps_list = existing_timestamps.tolist()\n",
    "    \n",
    "    for i in range(timestamps):\n",
    "        if i != 0 and i not in existing_timestamps_list:\n",
    "            new_row = group.iloc[0].copy()\n",
    "            new_row['timestamp'] = i\n",
    "            new_row['Time'] = None\n",
    "            new_row['HD mm'] =  np.nan\n",
    "            new_row['PR dm/min'] =  np.nan\n",
    "            new_row['HP bar'] =  np.nan\n",
    "            new_row['FP bar'] =  np.nan\n",
    "            new_row['DP bar'] =  np.nan\n",
    "            new_row['RS r/min'] =  np.nan\n",
    "            new_row['RP bar'] =  np.nan\n",
    "            new_row['WF l/min'] =  np.nan\n",
    "            new_row['WP bar'] =  np.nan\n",
    "            missing_rows.append(new_row)\n",
    "    \n",
    "    #information about borehole as well as all min/mean/max values that will be exactly the same in new rows\n",
    "    #time is not the same and will stay empty to later on see which data is interpolated an which is actual data\n",
    "    \n",
    "    interpolated_group = pd.concat([group, pd.DataFrame(missing_rows)]).sort_values('timestamp')\n",
    "    interpolated_group[['HD mm', 'PR dm/min', 'DP bar', 'RS r/min', 'RP bar', \n",
    "                       'HP bar', 'FP bar', 'WF l/min', 'WP bar']] = interpolated_group[['HD mm', 'PR dm/min', \n",
    "                                                                                        'DP bar', 'RS r/min', \n",
    "                                                                                        'RP bar', 'HP bar', \n",
    "                                                                                        'FP bar', 'WF l/min', \n",
    "                                                                                        'WP bar']].interpolate(method='linear')\n",
    "    \n",
    "    \n",
    "    return interpolated_group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_lstm = data_lstm.reset_index(drop=True)\n",
    "data_lstm = data_lstm.groupby(\"holeID\").apply(interpolate_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lstm = data_lstm.reset_index(drop=True)\n",
    "\n",
    "#drop if there are by any chance duplicates in timestamp (due to: date and time in multiple rows the same)\n",
    "data_lstm = data_lstm.drop_duplicates(subset=['timestamp', 'holeID'], keep='first')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use just test and train holes for further preparations as holeID 212359400 and 212359390 have an error when calc. timestamp\n",
    "\n",
    "train_lstm = pd.DataFrame(columns=data_lstm.columns)\n",
    "test_lstm = pd.DataFrame(columns=data_lstm.columns)\n",
    "\n",
    "def get_lstm_data(df, combinations_df):\n",
    "    filtered_df = pd.merge(df, combinations_df, on=['faceN', 'holeID'], how='inner')\n",
    "    return filtered_df\n",
    "\n",
    "train_lstm = get_lstm_data(data_lstm, train_holes)\n",
    "\n",
    "\n",
    "test_lstm = get_lstm_data(data_lstm, test_holes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data for VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data for VAE:\n",
    "data_vae = data_sorted.copy()\n",
    "\n",
    "#train test split for vae\n",
    "train_vae = pd.DataFrame(columns=data_vae.columns)\n",
    "test_vae = pd.DataFrame(columns=data_vae.columns)\n",
    "\n",
    "def get_vae_data(df, combinations_df):\n",
    "    #add just lines that have same faceN and holeID as in combinations df\n",
    "    filtered_df = pd.merge(df, combinations_df, on=['faceN', 'holeID'], how='inner')\n",
    "    #use just first row for each combination\n",
    "    filtered_df = filtered_df.groupby(['faceN', 'holeID']).first().reset_index()\n",
    "    return filtered_df\n",
    "\n",
    "#train_holes, test_holes were defined in earlier steps\n",
    "train_vae = get_vae_data(data_vae, train_holes)\n",
    "train_vae\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vae = get_vae_data(data_vae, test_holes)\n",
    "test_vae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# X.. dataset without target value dr_orig, Y.. just target value\n",
    "vaex_test = test_vae.drop(columns=['Unnamed: 0','explosives,kg/m3','faceN', 'Time', 'Section number * 1000',\n",
    "                                    'Hole number', 'Hole type', 'Boom', 'Date and time at rockcontact',\n",
    "                                    'location', 'holeID', 'Time_sec_mean', 'Time_sec_min',\n",
    "                                    'Time_sec_max', 'tunnel support']).reset_index(drop=True)\n",
    "vaey_test = test_vae['explosives,kg/m3']\n",
    "\n",
    "vaex_train = train_vae.drop(columns=['Unnamed: 0','explosives,kg/m3','faceN', 'Time', 'Section number * 1000',\n",
    "                                    'Hole number', 'Hole type', 'Boom', 'Date and time at rockcontact',\n",
    "                                    'location', 'holeID', 'Time_sec_mean', 'Time_sec_min',\n",
    "                                    'Time_sec_max', 'tunnel support']).reset_index(drop=True)\n",
    "vaey_train = train_vae['explosives,kg/m3']\n",
    "\n",
    "#keep names for columns\n",
    "cols = vaex_test.columns\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a single scaler as y and x will be combined for scaling \n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Concatenate training data, scale, and split\n",
    "combined_train = pd.concat([vaex_train, vaey_train], axis=1)\n",
    "combined_train_scaled = scaler.fit_transform(combined_train)\n",
    "print(combined_train_scaled.shape)\n",
    "\n",
    "# Split back into aex_train and aey_train\n",
    "vaex_train_scaled = combined_train_scaled[:, :vaex_train.shape[1]]\n",
    "vaey_train_scaled = combined_train_scaled[:, vaex_train.shape[1]:]\n",
    "\n",
    "# Do the same for test data\n",
    "combined_test = pd.concat([vaex_test, vaey_test], axis=1)\n",
    "combined_test_scaled = scaler.transform(combined_test) #not fit_transform!\n",
    "\n",
    "vaex_test_scaled = combined_test_scaled[:, :vaex_test.shape[1]]\n",
    "vaey_test_scaled = combined_test_scaled[:, vaex_test.shape[1]:]\n",
    "\n",
    "\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(vaex_train_scaled).float()\n",
    "y_train_tensor = torch.tensor(vaey_train_scaled).float()\n",
    "X_test_tensor = torch.tensor(vaex_test_scaled).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of VAE\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_size, latent_size): #no additional conditions implemented > more of a VAE\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 128), #fully connected layer\n",
    "            nn.ReLU(), # activation function, \n",
    "            nn.Linear(128, latent_size * 2)  # 2 * latent_size for mean and log-variance, to parameterize the latent space\n",
    "        )\n",
    "        \n",
    "        # decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_size),\n",
    "        )\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encode\n",
    "        enc_output = self.encoder(x)\n",
    "        mu, logvar = torch.chunk(enc_output, 2, dim=1)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "\n",
    "        # Decode\n",
    "        dec_output = self.decoder(z)\n",
    "\n",
    "        return dec_output, mu, logvar  #mean of latent space, log of variance of latent space\n",
    "\n",
    "# Dataset preparation\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, target):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "\n",
    "    # defines number of datarows\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    # define how to call a specific datarow/datapoint\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        y = self.target[idx]\n",
    "        return x, y\n",
    "\n",
    "# Hyperparameter\n",
    "input_size = vaex_train_scaled.shape[1]\n",
    "latent_size = 10  #if too high --> overfitting\n",
    "batch_size = 64\n",
    "num_epochs = 200\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# DataLoader\n",
    "train_dataset = CustomDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "# Modell initialisieren\n",
    "model = VAE(input_size, latent_size)\n",
    "\n",
    "# Optimizer and Loss-Funktion\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "#train vae\n",
    "for epoch in range(num_epochs):\n",
    "    for data, target in train_loader:\n",
    "        # Reset Gradienten\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward Pass\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "\n",
    "        # calculate losses\n",
    "        loss = criterion(recon_batch, data) + 0.5 * torch.sum(logvar.exp() - logvar - 1 + mu.pow(2))\n",
    "        \n",
    "        # Backward Pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    " \n",
    "    \n",
    "# Test VAE\n",
    "with torch.no_grad():\n",
    "    test_output, _, _ = model(X_test_tensor)\n",
    "    test_loss = criterion(test_output, X_test_tensor)\n",
    "    print(f\"Test Loss: {test_loss.item():.4f}\")\n",
    "\n",
    "combined = pd.concat([pd.DataFrame(test_output.detach().numpy()), vaey_test], axis=1)\n",
    "predicted_list = scaler.inverse_transform(combined)\n",
    "\n",
    "#create dataframe to compare results\n",
    "result_df = pd.DataFrame()\n",
    "result_df['originalExplosives'] = vaey_test\n",
    "predicted_list2 = predicted_list[:, pd.DataFrame(predicted_output.detach().numpy()).shape[1]]\n",
    "result_df['estimatedExplosives'] = predicted_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate MSE and RMSE\n",
    "\n",
    "mse_vae = mean_squared_error(result_df['originalExplosives'], result_df['estimatedExplosives'])\n",
    "rmse_vae = mse_vae**.5\n",
    "sym_mape_vae = smape(result_df['originalExplosives'], result_df['estimatedExplosives'])\n",
    "accuracy_vae = 100 - sym_mape_vae\n",
    "\n",
    "# result list\n",
    "result_data_list_vae = [\n",
    "        \"MSE {:.2f}\".format(mse_vae),\n",
    "        \"RMSE {:.2f}\".format(rmse_vae),\n",
    "        \"Acc {:.2f}\".format(accuracy_vae)\n",
    "        ]\n",
    "\n",
    "print(\"\\t\".join(result_data_list_vae))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a scatter plot\n",
    "plt.scatter(result_df['originalExplosives'], result_df['estimatedExplosives'])\n",
    "plt.title('Scatter Plot')\n",
    "plt.xlabel('original')\n",
    "plt.ylabel('predicted')\n",
    "plt.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create classes and compare results\n",
    "\n",
    "#assign classes to original as well as predicted values, classes are distributed linearly\n",
    "\n",
    "num_classes = 9  # (0.4 to 1.3 in 0.1-steps)\n",
    "\n",
    "# generate a threshold\n",
    "thresholds = np.linspace(0.4, 1.3, num_classes + 1)\n",
    "\n",
    "# calculate original classes\n",
    "result_df['orig_class'] = np.digitize(result_df['originalExplosives'], thresholds) - 1\n",
    "\n",
    "# calculate classes for prediction\n",
    "result_df['estim_class'] = np.digitize(result_df['estimatedExplosives'], thresholds) - 1\n",
    "\n",
    "print(result_df)\n",
    "\n",
    "#create confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "conf_matrix = confusion_matrix(result_df['orig_class'], result_df['estim_class'])\n",
    "#row 1 represents class 0, 0 times it was predicted to be class 0 and 1, once that it would be class 3, \n",
    "#so all in class 0 and 1 were predicted wrong\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',cbar=False)\n",
    "plt.title('Confusion Matrix:')\n",
    "plt.xlabel('prediction')\n",
    "plt.ylabel('actual class')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare individual results (precisioin, recall, f1, accuracy) for classes\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "classification_report_output = classification_report(result_df['orig_class'], result_df['estim_class']) \n",
    "\n",
    "print(classification_report_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Include LSTM in VAE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#final data preparatioin\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "#prepare data for scaling\n",
    "\n",
    "# drop unwanted columns, use for y just explosives,kg/m3 once for each sequence\n",
    "#lstmx_test_df is a dataframe grouped by holeID\n",
    "lstmx_test_df =(test_lstm.drop(columns=['Unnamed: 0','explosives,kg/m3','faceN', 'Time', 'Section number * 1000',\n",
    "                                      'Hole number', 'Hole type', 'Boom', 'Date and time at rockcontact',\n",
    "                                      'location', 'Time_sec_mean', 'Time_sec_min',\n",
    "                                      'Time_sec_max', 'tunnel support', 'timestamp'])).reset_index(drop=True).groupby('holeID')\n",
    "\n",
    "#change df into an array\n",
    "lstmx_test= [group.drop(columns=['holeID']).values for _, group in lstmx_test_df]\n",
    "\n",
    "#for y use just target value\n",
    "lstmy_test = test_lstm.groupby('holeID')['explosives,kg/m3'].first().reset_index()\n",
    "lstmy_test = lstmy_test['explosives,kg/m3'].to_numpy() #create array for scaling\n",
    "\n",
    "\n",
    "#same for train data\n",
    "\n",
    "lstmx_train_df = (train_lstm.drop(columns=['Unnamed: 0','explosives,kg/m3','faceN', 'Time', 'Section number * 1000',\n",
    "                                      'Hole number', 'Hole type', 'Boom', 'Date and time at rockcontact',\n",
    "                                      'location', 'Time_sec_mean', 'Time_sec_min',\n",
    "                                      'Time_sec_max', 'tunnel support', 'timestamp'])).reset_index(drop=True).groupby('holeID')\n",
    "\n",
    "lstmx_train = [group.drop(columns=['holeID']).values for _, group in lstmx_train_df]\n",
    "\n",
    "lstmy_train = train_lstm.groupby('holeID')['explosives,kg/m3'].first().reset_index()\n",
    "lstmy_train = lstmy_train['explosives,kg/m3'].to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#scale data (x and y indivicually)\n",
    "\n",
    "scaler_x = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "#fit x scaler to train data\n",
    "#genereate a dataframe that is not grouped yet and also has no holeID \n",
    "lstmx_train_fit_df = (train_lstm.drop(columns=['Unnamed: 0','explosives,kg/m3','faceN', 'Time', 'Section number * 1000',\n",
    "                                      'Hole number', 'Hole type', 'Boom', 'Date and time at rockcontact',\n",
    "                                      'location', 'Time_sec_mean', 'Time_sec_min',\n",
    "                                      'Time_sec_max', 'tunnel support', 'timestamp', 'holeID']))\n",
    "lstmx_train_fit = lstmx_train_fit_df.to_numpy() #data that will be transformed will also be array --> array without the groups\n",
    "\n",
    "#fit x scaler on this data\n",
    "lstmx_train_fit_scaled = scaler_x.fit_transform(lstmx_train_fit)\n",
    "\n",
    "\n",
    "def scale_timeseries(sequences, scaler):\n",
    "    return [scaler.transform(seq) for seq in sequences]\n",
    "\n",
    "\n",
    "#scale lstmx\n",
    "lstmx_train_scaled = scale_timeseries(lstmx_train, scaler_x)\n",
    "lstmx_test_scaled = scale_timeseries(lstmx_test, scaler_x)\n",
    "\n",
    "\n",
    "#scale lstmy (fit on train data)\n",
    "lstmy_train_scaled = scaler_y.fit_transform(lstmy_train.reshape(-1, 1))\n",
    "lstmy_test_scaled = scaler_y.transform(lstmy_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert scaled data in pytorch tensors\n",
    "#for y tensors\n",
    "y_train_tensor = torch.tensor(lstmy_train_scaled).float()\n",
    "y_test_tensor = torch.tensor(lstmy_test_scaled).float()\n",
    "\n",
    "\n",
    "#for x tensor\n",
    "#define max length of each timeseries\n",
    "max_train = max([len(group) for group in lstmx_train_scaled])\n",
    "max_test = max([len(group) for group in lstmx_test_scaled])\n",
    "max_seq_length = max(max_train, max_test)\n",
    "\n",
    "print(max_seq_length )\n",
    "\n",
    "#get train and test groups to same length (length of sequence depends on timesteps in original dataset)\n",
    "def adapt_sequence_length(group, max_length):\n",
    "    missing_steps = max_length - len(group)\n",
    "    if missing_steps > 0:\n",
    "        padded_group = np.pad(group, ((0, missing_steps), (0, 0)), mode='constant', constant_values=0)\n",
    "    else:\n",
    "        padded_group = group\n",
    "    return padded_group\n",
    "\n",
    "lstmx_train_padded = [adapt_sequence_length(group, max_seq_length) for group in lstmx_train_scaled]\n",
    "lstmx_test_padded = [adapt_sequence_length(group, max_seq_length) for group in lstmx_test_scaled]\n",
    "\n",
    "\n",
    "\n",
    "#expected form from model for input = [batch_size, seq_length, features]\n",
    "X_train_tensor = torch.tensor(lstmx_train_padded).float()\n",
    "X_test_tensor = torch.tensor(lstmx_test_padded).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up the model\n",
    "\n",
    "class VAELSTM(nn.Module):\n",
    "    def __init__(self, input_size, seq_length, latent_size, hidden_size=128, output_size=1):\n",
    "        super(VAELSTM, self).__init__()\n",
    "        \n",
    "        self.seq_length = seq_length\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.latent_size = latent_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # LSTM for encoding\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=2, batch_first=True) #True as data is [batch_size, seq_length, features]\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 128), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, latent_size * 2)  # Outputs mu and logvar\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, seq_length * input_size)  # Maps back to time series form\n",
    "        )\n",
    "        \n",
    "        # additional layer to transform output in desired output size (1 output per time series)\n",
    "        self.to_final_output = nn.Linear(seq_length * input_size, output_size)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        #encoder\n",
    "        _, (hidden, _) = self.lstm(x)  # Use only hidden state of the output (output, (hidden state, cell state))\n",
    "        hidden = hidden[-1, :, :]  # get last layer's hidden state\n",
    "        enc_out = self.encoder(hidden)\n",
    "        mu, logvar = torch.chunk(enc_out, 2, dim=1)  # Split into mu and logvar\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        \n",
    "        #decoder\n",
    "        dec_out = self.decoder(z)\n",
    "        dec_out = dec_out.view(-1, self.seq_length, self.input_size)  # Reshape to original data shape to [batch_size, seq_length, features]\n",
    "        final_out = self.to_final_output(dec_out.view(-1, self.seq_length * self.input_size))  # Reduce to final output size\n",
    "        \n",
    "        return final_out, mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameter\n",
    "input_size = X_train_tensor.size(2) #number of features in each step \n",
    "latent_size = 10 \n",
    "seq_length = X_train_tensor.size(1) \n",
    "hidden_size = 128\n",
    "output_size = 1\n",
    "batch_size = 64\n",
    "num_epochs = 200\n",
    "learning_rate = 1e-3\n",
    "\n",
    "\n",
    "# DataLoader \n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.labels[index]\n",
    "\n",
    "train_dataset = CustomDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "\n",
    "model = VAELSTM(input_size, seq_length, latent_size, hidden_size, output_size)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training \n",
    "for epoch in range(num_epochs):\n",
    "    for data, target in train_loader:\n",
    "        # Reset Gradienten\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward Pass\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "\n",
    "        # calculate losses\n",
    "        loss = criterion(recon_batch, target) + 0.5 * torch.sum(logvar.exp() - logvar - 1 + mu.pow(2))\n",
    "        \n",
    "        # Backward Pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    " # Test LSTM\n",
    "with torch.no_grad():\n",
    "    test_output, _, _ = model(X_test_tensor)\n",
    "\n",
    "    test_loss = criterion(test_output, y_test_tensor)\n",
    "    print(f\"Test Loss: {test_loss.item():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inverse scaling\n",
    "output_unscaled = scaler_y.inverse_transform(test_output)\n",
    "output_unscaled_df = pd.DataFrame(output_unscaled, columns=[f'predicted' for i in range(output_unscaled.shape[1])])\n",
    "\n",
    "original_df = pd.DataFrame(lstmy_test, columns=['original'])\n",
    "\n",
    "\n",
    "result_df = pd.concat([original_df, output_unscaled_df], axis=1)\n",
    "\n",
    "result_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate MSE and RMSE\n",
    "\n",
    "mse_lstm = mean_squared_error(result_df['original'], result_df['predicted'])\n",
    "rmse_lstm = mse_lstm**.5\n",
    "sym_mape_lstm = smape(result_df['original'], result_df['predicted'])\n",
    "accuracy_lstm = 100 - sym_mape_lstm\n",
    "\n",
    "# results\n",
    "result_data_list_lstm = [\n",
    "        \"MSE {:.2f}\".format(mse_lstm),\n",
    "        \"RMSE {:.2f}\".format(rmse_lstm),\n",
    "        \"Acc {:.2f}\".format(accuracy_lstm)\n",
    "        ]\n",
    "\n",
    "print(\"\\t\".join(result_data_list_lstm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check with classification again\n",
    "\n",
    "num_classes = 9  # (0.4 bis 1.3 in 0.1-steps)\n",
    "\n",
    "# generale threshhold\n",
    "thresholds = np.linspace(0.4, 1.3, num_classes + 1)\n",
    "\n",
    "#calculate classes \n",
    "result_df['orig_class'] = np.digitize(result_df['original'], thresholds) - 1\n",
    "result_df['estim_class'] = np.digitize(result_df['predicted'], thresholds) - 1\n",
    "\n",
    "print(result_df)\n",
    "\n",
    "#create confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "conf_matrix = confusion_matrix(result_df['orig_class'], result_df['estim_class'])\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',cbar=False)\n",
    "plt.title('Confusion Matrix:')\n",
    "plt.xlabel('prediction')\n",
    "plt.ylabel('actual class')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
